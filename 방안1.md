목표는 **Heroku Connect의 '실시간/준실시간 양방향 동기화' 기능을 저비용으로 우리만의 커넥터로 대체하는 것**이 핵심이겠네요! 🛠️💰

그럼, Heroku Connect를 대체하면서 비용 효율적이고 확장성 있는 최적의 방안을 제안해 드릴게요! ✨

---

### 🚀 **최적의 방안: 이벤트 기반 아키텍처 + 서버리스 함수 (혹은 경량화된 Worker)**

Heroku Connect의 실시간 동기화 특성을 대체하려면, "데이터 변경 이벤트"에 즉시 반응하는 **이벤트 기반 아키텍처(Event-Driven Architecture)**가 가장 효과적이에요. 여기에 서버리스 함수를 결합하면 운영 오버헤드와 비용을 크게 줄일 수 있어요!

#### 1. ⚙️ 전체적인 아키텍처 컨셉

*   **변경 감지 (Change Data Capture, CDC)**: Salesforce와 PostgreSQL 양쪽에서 데이터 변경을 감지합니다.
*   **이벤트 발행**: 감지된 변경 사항을 이벤트(메시지) 형태로 발행합니다.
*   **이벤트 큐 (선택 사항)**: 발행된 이벤트를 안전하고 비동기적으로 처리하기 위해 메시지 큐(예: AWS SQS, Apache Kafka, RabbitMQ)를 사용할 수 있어요. 규모가 작으면 굳이 필요 없을 수도 있지만, 견고함을 더해줍니다.
*   **이벤트 컨슈머 (서버리스 함수/Worker)**: 메시지 큐나 변경 이벤트를 구독하여 해당 이벤트를 처리하고 대상 시스템(Salesforce 또는 PostgreSQL)에 반영합니다.
*   **에러 처리 및 재시도**: 실패한 이벤트를 다시 처리하거나 로깅하여 데이터 불일치를 방지합니다.

#### 2. ➡️ **Salesforce 변경사항 -> Heroku PostgreSQL 동기화**

Salesforce에서 데이터가 변경되었을 때, 이를 PostgreSQL로 푸시하는 방법이에요.

*   **① Salesforce 측 변경 감지:**
    *   **Salesforce Platform Events 또는 Change Data Capture (CDC) Events:** Salesforce는 플랫폼 레코드 변경 사항을 실시간으로 게시하는 기능을 제공해요. 이걸 구독해서 변경 이벤트를 받아볼 수 있어요. 이것이 가장 "Salesforce Native"하고 효율적인 방법입니다.
    *   **Salesforce Outbound Message (워크플로우/플로우 기반):** 특정 객체의 변경이 있을 때 외부 엔드포인트(URL)로 XML 메시지를 보내는 방식이에요. 설정이 간편하지만, 메시지 내용이나 포맷 제약이 좀 있어요.
    *   **Apex Trigger + HTTP Callout:** 가장 유연하지만, Apex 코드 작성 및 관리 부담, 그리고 Salesforce HTTP 콜아웃 제한(Callout Limit)을 고려해야 해요. 대량 업데이트 시 성능 문제가 발생할 수 있어요.

*   **② 이벤트 처리 및 PostgreSQL 반영:**
    *   **Heroku에서 실행되는 Serverless Function 또는 Worker:**
        *   **Serverless (e.g., AWS Lambda, Google Cloud Functions):** Salesforce 이벤트 구독/Outbound Message 엔드포인트 역할을 하는 HTTP Endpoint를 만들고, 이 함수가 Salesforce로부터 이벤트를 받으면 Heroku PostgreSQL에 데이터를 삽입/업데이트합니다. 비용 효율적이고 확장성이 좋아요. Heroku 대신 AWS/GCP에서 함수를 호스팅해도 됩니다.
        *   **Heroku Worker Dyno:** Python, Node.js 등으로 이벤트를 처리하는 워커 프로세스를 Heroku Dyno에서 상시 실행할 수도 있어요. 하지만 서버리스보다 비용 효율성이 떨어질 수 있습니다.
        *   **데이터 반영 로직**: 받아온 Salesforce 데이터(JSON 형태)를 파싱해서 PostgreSQL 테이블에 UPSERT (INSERT ON CONFLICT UPDATE) 쿼리를 실행합니다.

#### 3. ⬅️ **Heroku PostgreSQL 변경사항 -> Salesforce 동기화**

PostgreSQL에서 데이터가 변경되었을 때, 이를 Salesforce로 푸시하는 방법이에요.

*   **① PostgreSQL 측 변경 감지:**
    *   **PostgreSQL Trigger + Notification:** PostgreSQL 테이블에 `INSERT`, `UPDATE`, `DELETE` 트랜잭션이 발생할 때마다 트리거를 걸어서 `NOTIFY` 명령을 통해 특정 채널로 알림을 보냅니다.
    *   **Logical Replication / WAL (Write-Ahead Log) 기반 CDC 툴 (좀 더 고급):** Debezium 같은 CDC 도구를 사용하면 PostgreSQL의 WAL을 읽어서 데이터 변경 이벤트를 실시간으로 스트리밍할 수 있어요. 이는 매우 강력하고 안정적이지만, 설정 및 운영 복잡도가 높습니다. 작은 규모에서는 Trigger가 더 현실적일 수 있어요.

*   **② 이벤트 처리 및 Salesforce 반영:**
    *   **Heroku에서 실행되는 Serverless Function 또는 Worker:**
        *   **Serverless (e.g., AWS Lambda, Google Cloud Functions):** PostgreSQL `NOTIFY` 이벤트를 리슨하거나 (직접 리스닝은 어려울 수 있어 메시지 큐를 통해) 또는 주기적으로 `last_modified_date` 등을 통해 변경된 데이터를 확인해서 Salesforce API를 호출해 업데이트합니다.
        *   **Heroku Worker Dyno:** PostgreSQL `LISTEN` 명령을 사용하여 알림을 실시간으로 받아서 처리하고 Salesforce API를 호출하는 워커 프로세스를 상시 실행합니다.
        *   **데이터 반영 로직**: Salesforce REST API의 `upsert` 기능을 활용하여 외부 ID(External ID) 필드를 기준으로 Salesforce 객체를 업데이트합니다.

#### 4. 🔗 **최적의 조합 (초기 로딩 + 실시간 동기화)**

*   **초기 데이터 로드 (Initial Data Load):**
    *   이건 **Airflow DAG**가 여전히 최고의 선택입니다. 대량의 데이터를 안정적으로 추출하고 변환해서 한 번에 밀어 넣는 데 특화되어 있어요.
    *   Airflow의 강력한 재시도, 모니터링, 오류 처리 기능은 초기 로드 시 발생할 수 있는 다양한 문제를 해결하는 데 큰 도움이 됩니다.
*   **지속적인 변경 동기화 (Ongoing Change Sync):**
    *   위에서 설명한 **이벤트 기반 아키텍처 + 서버리스 함수(혹은 경량화된 Heroku Worker)**를 활용하여 실시간에 가까운 동기화를 구현합니다.

---

### 🤔 왜 이 방안이 "최적"일까?

1.  **비용 효율성 💰:**
    *   서버리스 함수는 사용한 만큼만 비용을 지불하므로, Heroku Connect의 고정 비용보다 훨씬 저렴할 수 있어요 (특히 동기화되는 변경량이 많지 않을 때).
    *   Airflow 클러스터 유지 보수 비용도 초기 로딩 시에만 집중하고, 실시간 부분은 경량화하여 절감할 수 있어요.
2.  **확장성 & 안정성 📈:**
    *   서버리스 함수는 자동으로 스케일 아웃되므로, 갑자기 동기화할 데이터가 폭증해도 안정적으로 처리할 수 있어요.
    *   이벤트 큐를 사용하면 메시지 손실 없이 견고하게 시스템을 구축할 수 있습니다.
3.  **유연성 & 제어 🎛️:**
    *   커스텀 코드를 작성하므로, 스키마 매핑, 데이터 변환, 충돌 처리 로직 등을 우리  환경에 맞춰 완벽하게 제어하고 최적화할 수 있어요.
    *   (이전 답변에서 얘기했던) 스키마 자동 매핑이나 규칙 기반 변환 로직도 이 서버리스 함수 안에 구현할 수 있구요!
4.  **Airflow의 강점 활용 💨:**
    *   대규모 초기 로딩이나 주기적인 정합성 검사 등 배치 처리가 필요한 부분에서는 여전히 Airflow를 활용하여 각자의 장점을 극대화할 수 있습니다.

---

### 📝뭘 선택해야 할까?

Heroku에서 호스팅하는 환경이라면,

*   **PostgreSQL -> Salesforce**: PostgreSQL의 Trigger + Heroku Worker Dyno(혹은 Heroku Runtime API를 통해 함수를 직접 호스팅)가 가장 Heroku 친화적인 방법일 것 같아요.
*   **Salesforce -> PostgreSQL**: Salesforce Outbound Message나 Platform Events를 받아서 Heroku Worker Dyno에서 처리하거나, AWS Lambda 같은 외부 Serverless 함수를 호출해서 처리할 수도 있구요. (이벤트 구독은 보통 외부 서비스에서 많이 해요)

어때요, ! 단순히 Airflow만 쓰는 것보다는 Heroku Connect가 제공하던 "실시간"의 맛을 살리면서 비용도 절감할 수 있는 좋은 전략이 될 것 같지 않아요? 😊 물론 이 방법을 택하면 초기 설계와 구현은 좀 더 복잡해지겠지만, 장기적으로 봤을 때 훨씬 강력하고 효율적인 솔루션이 될 거예요!

혹시 "이벤트 큐는 뭘 써야 할까요?", "어떤 서버리스 함수가 좋을까요?" 같은 궁금증이 있다면, 제가 또 자세히 설명해 드릴 수 있어요! 💖 어떤 방향으로든 제가 열심히 도와드릴게요! 파이팅입니다!! 

참고 자료 

[1] www.heroku.com - Heroku Connect: Sync Data Between Postgres and Salesforce (https://www.heroku.com/connect/)
[2] Adaptid - How to Sync Salesforce Data with Heroku in Minutes - Adaptid (https://adaptid.com/quick-guide-to-heroku-sync-for-salesforce-users/)
[3] devcenter.heroku.com - Integrating Heroku and the Salesforce Platform Overview (https://devcenter.heroku.com/articles/integrating-heroku-and-salesforce)
[4] Heroku - Salesforce Data Migration Team - Heroku (https://www.heroku.com/customers/salesforce-data-migration-team/)
[5] Postgres Synchronisation | by Gerrit Riessen - The Case For Salesforce-Postgres Synchronisation | by Gerrit Riessen (https://gorenje.medium.com/the-case-for-bi-directional-salesforce-postgres-synchronization-ff0090ef6531)

---
와 , 아주 좋습니다! "Heroku 환경은 고려하지 않는다"고 딱 정해주시니, 더 넓은 시야에서 최적의 이벤트 기반 아키텍처를 설계하고 예제 코드를 만들어볼 수 있겠네요! 💪

Heroku Connect의 빈자리를 우리 손으로 채우는 것! 진짜 멋진 목표예요. 제가 제시한 **이벤트 기반 아키텍처 + 서버리스 함수**는 이런 목표에 딱 맞는 방식이 될 거예요. 개발 비용은 확 줄이고, 확장성과 유연성은 극대화할 수 있습니다.

여기서는 일반적인 서버리스 환경(예: AWS Lambda)을 가정하고 예제 코드를 작성해볼게요. 필요한 라이브러리 설치와 환경 변수 설정만 잘 해주면 어떤 서버리스 플랫폼에서도 유사하게 작동할 수 있을 거예요.

---

### ✨ **전체 아키텍처 재확립**

Heroku를 제외한 더 일반적인 시나리오로 다시 정리해볼게요.

1.  **Salesforce -> PostgreSQL 동기화 흐름:**
    *   **Salesforce:**
        *   **CDC Events (Change Data Capture)** 또는 **Platform Events**를 활성화합니다.
        *   외부 웹훅(Webhook) 서비스를 통해 특정 HTTP 엔드포인트(서버리스 함수)로 이벤트 발생 시 데이터를 푸시하도록 설정합니다. (예: AWS EventBridge + Lambda, GCP Pub/Sub + Cloud Functions)
    *   **서버리스 함수 1 (`SF_to_PG_Sync_Function`):**
        *   Salesforce로부터 JSON 형태의 변경 이벤트를 수신합니다.
        *   이벤트 데이터를 파싱하여 PostgreSQL 연결 후 UPSERT (INSERT ON CONFLICT UPDATE) 작업을 수행합니다.
    *   **PostgreSQL:** 변경된 데이터가 반영됩니다.

2.  **PostgreSQL -> Salesforce 동기화 흐름:**
    *   **PostgreSQL:**
        *   **Change Log Table + Trigger**: 대상 테이블에 변경이 발생할 때마다 트랜잭션 단위로 변경 전/후 데이터를 `change_log` 테이블에 기록하는 트리거를 생성합니다.
        *   (`Optional: Message Queue`): `change_log` 테이블에 데이터가 기록되면, 별도의 프로세스(예: Debezium과 같은 CDC 툴)가 이를 감지하여 메시지 큐(예: AWS SQS, Apache Kafka)로 변경 이벤트를 발행합니다. 또는 간소화를 위해 서버리스 함수가 `change_log` 테이블을 주기적으로 폴링할 수도 있습니다.
    *   **서버리스 함수 2 (`PG_to_SF_Sync_Function`):**
        *   메시지 큐로부터 변경 이벤트를 수신하거나, `change_log` 테이블을 주기적으로 쿼리하여 변경된 데이터를 가져옵니다.
        *   PostgreSQL 데이터를 파싱하여 Salesforce 연결 후 Upsert 작업을 수행합니다.
    *   **Salesforce:** 변경된 데이터가 반영됩니다.

---

### 💻 **양방향 예제 코드**

아래 코드는 두 개의 독립적인 서버리스 함수 예시입니다. 각 함수는 해당 시스템의 변경 이벤트를 처리합니다.

#### 📦 **필수 라이브러리 설치 (예시)**

`pip install simple-salesforce psycopg2-binary pandas`
이 라이브러리들은 서버리스 환경의 배포 패키지에 포함되어야 합니다.

#### ⚙️ **공통 설정 (환경 변수로 관리)**

보안과 유연성을 위해 모든 민감 정보는 환경 변수로 관리하는 것을 강력히 추천합니다.

*   `SF_USERNAME`: Salesforce 사용자 이름
*   `SF_PASSWORD`: Salesforce 비밀번호
*   `SF_SECURITY_TOKEN`: Salesforce 보안 토큰
*   `SF_DOMAIN`: Salesforce 도메인 (예: `login.salesforce.com` 또는 커스텀 도메인)
*   `SF_EXTERNAL_ID_FIELD`: Salesforce에서 사용할 외부 ID 필드 이름 (예: `External_Id__c`)

*   `PG_HOST`: PostgreSQL 호스트
*   `PG_DBNAME`: PostgreSQL 데이터베이스 이름
*   `PG_USER`: PostgreSQL 사용자 이름
*   `PG_PASSWORD`: PostgreSQL 비밀번호
*   `PG_PORT`: PostgreSQL 포트 (기본값: 5432)

---

### ➡️ **1. Salesforce -> PostgreSQL 동기화 함수 예제**
(함수명: `SF_to_PG_Sync_Function`)

이 함수는 Salesforce CDC 이벤트 또는 Outbound Message를 웹훅으로 수신할 것으로 가정합니다.

```python
# salesforce_to_postgresql_function.py
import os
import json
import psycopg2
from simple_salesforce import Salesforce # Salesforce API 호출에 사용되지만, 이 방향에서는 이벤트 수신용이므로 직접 사용하지 않을 수도 있음
import pandas as pd # 데이터 처리를 좀 더 유연하게 하기 위해

# --- 환경 변수 로드 ---
PG_HOST = os.environ.get('PG_HOST')
PG_DBNAME = os.environ.get('PG_DBNAME')
PG_USER = os.environ.get('PG_USER')
PG_PASSWORD = os.environ.get('PG_PASSWORD')
PG_PORT = os.environ.get('PG_PORT', '5432')

# 스키마 매핑 헬퍼 함수 (이전 답변에서 제시한 것)
def sf_field_to_pg_column_name(sf_field_name: str) -> str:
    import re
    name = sf_field_name.replace('__c', '')
    name = re.sub(r'(?<!^)(?=[A-Z])', '_', name).lower()
    name = re.sub(r'_{2,}', '_', name).strip('_')
    return name

def handler(event, context):
    """
    Salesforce CDC 또는 Outbound Message 이벤트를 받아 PostgreSQL에 반영하는 서버리스 함수.
    이벤트 형식은 사용하는 Salesforce 웹훅/이벤트 유형에 따라 달라질 수 있습니다.
    여기서는 CDC Event ChangeDataCaptureEvent 스키마를 가정합니다.
    """
    print(f"Salesforce -> PostgreSQL Sync Function Triggered!")
    print(f"Received Event: {json.dumps(event, indent=2)}")

    pg_conn = None
    try:
        # --- 1. 이벤트 데이터 파싱 ---
        # Salesforce CDC 이벤트는 Record `payload` 안에 변경된 데이터가 있어요.
        # 실제 이벤트 구조에 따라 파싱 로직을 조정해야 합니다.
        if 'cdc' in event: # AWS EventBridge의 Salesforce CDC source를 가정
            # event 구조 예시: {"cdc": {"schema": "...", "payload": {"Id": "...", "Name": "...", ...}}}
            change_data = event['cdc']['payload']
            object_name = event['cdc']['schema'].split('/')[-1].split('ChangeEvent')[0] # 'AccountChangeEvent' -> 'Account'
            
        elif 'Records' in event and event['Records'][0].get('EventSource') == 'aws:sqs': # SQS에서 메시지를 받는 경우
            # SQS 메시지 바디는 보통 JSON 문자열
            body = json.loads(event['Records'][0]['body'])
            # 이 SQS 메시지 바디 안에 Salesforce 이벤트 페이로드가 있다고 가정
            change_data = body['payload'] 
            object_name = body['object_name'] # 메시지 보낼 때 object_name 필드를 추가했다고 가정

        else: # 직접적인 웹훅 호출 또는 테스트용 JSON을 가정
            change_data = event.get('payload') # 혹은 이벤트의 루트에 payload가 있을 수 있음
            object_name = event.get('object_name') # 어떤 객체의 변경인지 명시해줘야 함 (예: 'Account')

        if not change_data or not object_name:
            print("ERROR: Could not parse change data or object name from event.")
            return {
                'statusCode': 400,
                'body': json.dumps({'message': 'Invalid event payload'})
            }

        sf_id = change_data.get('Id')
        if not sf_id:
            print("ERROR: Salesforce 'Id' not found in change data. Skipping.")
            return {
                'statusCode': 400,
                'body': json.dumps({'message': 'Salesforce Id missing'})
            }

        pg_table_name = sf_field_to_pg_column_name(object_name) # 'Account' -> 'account'
        pg_columns = []
        pg_values = []
        pg_update_set = []

        # Salesforce 데이터 필드를 PostgreSQL 컬럼과 값으로 변환
        for sf_field, sf_value in change_data.items():
            if sf_field == 'attributes': # SF API 응답에서 attributes 필드는 스킵
                continue
            
            pg_col = sf_field_to_pg_column_name(sf_field)
            pg_columns.append(pg_col)
            pg_values.append(sf_value)
            if pg_col != 'id': # 'id'는 PK이므로 UPDATE SET에 포함하지 않음
                pg_update_set.append(f"{pg_col} = %s")

        # --- 2. PostgreSQL 연결 ---
        pg_conn = psycopg2.connect(
            host=PG_HOST,
            dbname=PG_DBNAME,
            user=PG_USER,
            password=PG_PASSWORD,
            port=PG_PORT
        )
        pg_cursor = pg_conn.cursor()

        # --- 3. PostgreSQL에 UPSERT ---
        # 'id' 컬럼을 기준으로 ON CONFLICT DO UPDATE
        insert_cols = ", ".join(pg_columns)
        value_placeholders = ", ".join(['%s'] * len(pg_columns))
        update_cols = ", ".join(pg_update_set)

        upsert_query = f"""
            INSERT INTO {pg_table_name} ({insert_cols})
            VALUES ({value_placeholders})
            ON CONFLICT (id) DO UPDATE SET
                {update_cols};
        """
        # ID를 첫 번째 매개변수로 사용하여 UPDATE SET의 WHERE 절에 일치
        # 그리고 INSERT VALUES에도 사용
        # psycopg2는 자동으로 튜플을 인식
        
        # pg_values 리스트에 id 값이 포함되어 있고, update_set에는 id가 없으므로 
        # UPSERT 쿼리 파라미터 구성 시, INSERT VALUES에 사용될 값들과 UPDATE SET에 사용될 값들을 분리해야 합니다.
        
        # update_set에 들어갈 값들을 추출
        update_values_for_set = []
        for sf_field, sf_value in change_data.items():
            if sf_field != 'Id' and sf_field != 'attributes':
                 update_values_for_set.append(sf_value)
        
        # 실행할 파라미터는 (insert_values) + (update_values_for_set)
        # INSERT ... VALUES (...) ON CONFLICT (id) DO UPDATE SET field1=%s, field2=%s ...
        # (v1, v2, v3, ...) + (v2, v3, ...)
        
        params = tuple(pg_values) + tuple(update_values_for_set)

        print(f"Executing UPSERT for {object_name} (Id: {sf_id}) to {pg_table_name}")
        pg_cursor.execute(upsert_query, params)
        pg_conn.commit()

        print(f"SUCCESS: Salesforce '{object_name}' (Id: {sf_id}) synchronized to PostgreSQL '{pg_table_name}'.")

        return {
            'statusCode': 200,
            'body': json.dumps({'message': 'Synchronization successful'})
        }

    except psycopg2.Error as e:
        print(f"PostgreSQL Error: {e}")
        if pg_conn:
            pg_conn.rollback() # 오류 시 롤백
        return {
            'statusCode': 500,
            'body': json.dumps({'message': f'PostgreSQL Error: {str(e)}'})
        }
    except Exception as e:
        print(f"General Error: {e}")
        return {
            'statusCode': 500,
            'body': json.dumps({'message': f'Internal Server Error: {str(e)}'})
        }
    finally:
        if pg_conn:
            pg_cursor.close()
            pg_conn.close()

```

#### ⬅️ **2. PostgreSQL -> Salesforce 동기화 함수 예제**
(함수명: `PG_to_SF_Sync_Function`)

이 함수는 `change_log` 테이블의 변경 이벤트를 처리합니다. 가장 간단한 방법으로 **정기적으로 `change_log` 테이블을 폴링**하여 처리하는 시나리오로 만들어볼게요. 실제 운영에서는 `change_log`가 메시지 큐로 데이터를 보내고, 함수는 큐에서 메시지를 소비하는 것이 더 효율적입니다.

```python
# postgresql_to_salesforce_function.py
import os
import json
import psycopg2
from simple_salesforce import Salesforce, SalesforceError

# --- 환경 변수 로드 ---
SF_USERNAME = os.environ.get('SF_USERNAME')
SF_PASSWORD = os.environ.get('SF_PASSWORD')
SF_SECURITY_TOKEN = os.environ.get('SF_SECURITY_TOKEN')
SF_DOMAIN = os.environ.get('SF_DOMAIN', 'login.salesforce.com')
SF_EXTERNAL_ID_FIELD = os.environ.get('SF_EXTERNAL_ID_FIELD', 'External_Id__c') # Salesforce 객체의 External ID 필드명

PG_HOST = os.environ.get('PG_HOST')
PG_DBNAME = os.environ.get('PG_DBNAME')
PG_USER = os.environ.get('PG_USER')
PG_PASSWORD = os.environ.get('PG_PASSWORD')
PG_PORT = os.environ.get('PG_PORT', '5432')

# 스키마 매핑 헬퍼 함수
def pg_column_to_sf_field_name(pg_column_name: str) -> str:
    # 예시: account_name -> AccountName
    # 여기서는 간단히 언더스코어를 제거하고 각 단어의 첫 글자를 대문자로
    return "".join(word.capitalize() for word in pg_column_name.split('_'))

def handler(event, context):
    """
    PostgreSQL change_log 테이블을 폴링하여 Salesforce에 변경 사항을 반영하는 서버리스 함수.
    이 함수는 예를 들어 CloudWatch Event (AWS Lambda) 등으로 주기적으로 실행될 수 있습니다.
    """
    print(f"PostgreSQL -> Salesforce Sync Function Triggered!")

    pg_conn = None
    sf_conn = None
    try:
        # --- 1. PostgreSQL 연결 및 변경 로그 조회 ---
        pg_conn = psycopg2.connect(
            host=PG_HOST,
            dbname=PG_DBNAME,
            user=PG_USER,
            password=PG_PASSWORD,
            port=PG_PORT
        )
        pg_cursor = pg_conn.cursor()

        # 예시: 'change_log' 테이블에서 아직 처리되지 않은 변경사항 조회
        # change_log 테이블은 다음과 같은 구조를 가정합니다:
        # id (PK), table_name, record_id (변경된 레코드의 PK), change_type (INSERT/UPDATE/DELETE),
        # old_data (JSON), new_data (JSON), processed (BOOLEAN), created_at
        
        # 주의: 여기서는 `record_id`가 Salesforce의 `External_Id__c`와 매핑될 것으로 가정합니다.
        # 즉, PostgreSQL의 ID와 Salesforce의 External ID가 같은 값을 가진다고 가정해요.
        
        pg_cursor.execute("""
            SELECT id, table_name, record_id, change_type, new_data 
            FROM change_log 
            WHERE processed = FALSE 
            ORDER BY created_at ASC 
            LIMIT 100; -- 한 번에 처리할 레코드 수 제한
        """)
        change_records = pg_cursor.fetchall()

        if not change_records:
            print("No new change records to process in PostgreSQL.")
            return {
                'statusCode': 200,
                'body': json.dumps({'message': 'No changes processed'})
            }

        # --- 2. Salesforce 연결 ---
        sf_conn = Salesforce(
            username=SF_USERNAME,
            password=SF_PASSWORD,
            security_token=SF_SECURITY_TOKEN,
            domain=SF_DOMAIN
        )
        print("Connected to Salesforce successfully!")

        processed_log_ids = []
        for log_id, table_name, record_id, change_type, new_data_json in change_records:
            print(f"Processing change log ID: {log_id} (Table: {table_name}, Record ID: {record_id}, Type: {change_type})")

            # JSONB 컬럼에서 데이터 로드
            try:
                # new_data_json은 JSON 문자열이어야 함
                new_data = json.loads(new_data_json) if isinstance(new_data_json, str) else new_data_json
            except (json.JSONDecodeError, TypeError) as e:
                print(f"ERROR: Could not decode JSON data for log ID {log_id}: {e}")
                continue

            sf_object_name = pg_column_to_sf_field_name(table_name) # 'account' -> 'Account'

            # Salesforce에 전송할 레코드 준비
            sf_record_payload = {}
            for pg_col, pg_val in new_data.items():
                if pg_col == 'id': # 'id' 컬럼은 Salesforce의 External ID로 사용
                    continue
                sf_field = pg_column_to_sf_field_name(pg_col)
                sf_record_payload[sf_field] = pg_val
            
            # External ID 필드를 payload에 추가
            sf_record_payload[SF_EXTERNAL_ID_FIELD] = record_id

            try:
                # Salesforce API Upsert 호출 (External ID를 사용하여 존재하면 업데이트, 없으면 생성)
                # simple-salesforce는 해당 메서드를 제공
                result = sf_conn.restful(
                    f"sobjects/{sf_object_name}/{SF_EXTERNAL_ID_FIELD}/{record_id}", 
                    method='PATCH', # PATCH는 Upsert와 유사한 동작을 함
                    data=sf_record_payload
                )
                print(f"Salesforce Upsert Result for {sf_object_name} (External ID: {record_id}): {result}")
                
                # 결과 확인
                if 'success' in result and result['success'] == True:
                    processed_log_ids.append(log_id)
                elif 'errors' in result and result['errors']:
                     print(f"ERROR: Salesforce Upsert failed for Log ID {log_id}: {result['errors']}")
                else: # PATCH 메서드는 성공 시 보통 빈 딕셔너리를 반환할 수 있음
                    print(f"INFO: Salesforce Upsert completed (no explicit 'success' key for Log ID {log_id}). Assuming success.")
                    processed_log_ids.append(log_id)


            except SalesforceError as e:
                print(f"ERROR: Salesforce API Error for Log ID {log_id}: {e.content}")
                # 필요한 경우 에러 로그를 남기거나 알림
            except Exception as e:
                print(f"ERROR: General Error during Salesforce Upsert for Log ID {log_id}: {e}")
                # 다른 예외 처리

        # --- 3. 처리된 변경 로그 'processed' 상태 업데이트 ---
        if processed_log_ids:
            update_query = f"""
                UPDATE change_log 
                SET processed = TRUE, processed_at = NOW() 
                WHERE id IN ({', '.join(['%s'] * len(processed_log_ids))});
            """
            pg_cursor.execute(update_query, tuple(processed_log_ids))
            pg_conn.commit()
            print(f"Marked {len(processed_log_ids)} change logs as processed.")
        
        return {
            'statusCode': 200,
            'body': json.dumps({'message': f'Processed {len(processed_log_ids)} changes.'})
        }

    except psycopg2.Error as e:
        print(f"PostgreSQL Error: {e}")
        if pg_conn:
            pg_conn.rollback()
        return {
            'statusCode': 500,
            'body': json.dumps({'message': f'PostgreSQL Error: {str(e)}'})
        }
    except SalesforceError as e:
        print(f"Salesforce API Connection Error: {e.content}")
        return {
            'statusCode': 500,
            'body': json.dumps({'message': f'Salesforce Connection Error: {str(e.content)}'})
        }
    except Exception as e:
        print(f"General Error: {e}")
        return {
            'statusCode': 500,
            'body': json.dumps({'message': f'Internal Server Error: {str(e)}'})
        }
    finally:
        if pg_conn:
            pg_cursor.close()
            pg_conn.close()
        # Salesforce 연결은 세션 기반이라 명시적 close는 불필요할 수 있음
```

---

### 💡 **구현 전 중요하게 체크할 것들!**

1.  **Salesforce External ID**: PostgreSQL -> Salesforce 동기화를 위해서는 Salesforce 객체에 `External_Id__c`와 같은 **외부 ID 필드**가 반드시 정의되어 있어야 합니다. 이 필드는 중복되지 않는 값이어야 해요.
2.  **PostgreSQL `change_log` 테이블 및 트리거**:
    *   `change_log` 테이블을 미리 생성해야 합니다.
        ```sql
        CREATE TABLE change_log (
            id BIGSERIAL PRIMARY KEY,
            table_name TEXT NOT NULL,       -- 변경된 테이블 이름 (예: 'account')
            record_id TEXT NOT NULL,        -- 변경된 레코드의 PRIMARY KEY (Salesforce External ID와 매핑)
            change_type TEXT NOT NULL,      -- 'INSERT', 'UPDATE', 'DELETE'
            old_data JSONB,                 -- 변경 전 데이터 (DELETE나 UPDATE의 경우)
            new_data JSONB,                 -- 변경 후 데이터 (INSERT나 UPDATE의 경우)
            processed BOOLEAN DEFAULT FALSE,-- 처리 여부 플래그
            processed_at TIMESTAMP,
            created_at TIMESTAMP DEFAULT NOW()
        );
        ```
    *   대상 테이블 (예: `account`)에 `AFTER INSERT OR UPDATE OR DELETE` 트리거를 만들어서 `change_log` 테이블에 데이터를 기록하도록 해야 합니다.
        ```sql
        -- 예시: account 테이블에 대한 트리거 함수
        CREATE OR REPLACE FUNCTION log_account_changes()
        RETURNS TRIGGER AS $$
        BEGIN
            IF (TG_OP = 'INSERT') THEN
                INSERT INTO change_log (table_name, record_id, change_type, new_data)
                VALUES ('account', NEW.id, 'INSERT', to_jsonb(NEW));
                RETURN NEW;
            ELSIF (TG_OP = 'UPDATE') THEN
                INSERT INTO change_log (table_name, record_id, change_type, old_data, new_data)
                VALUES ('account', NEW.id, 'UPDATE', to_jsonb(OLD), to_jsonb(NEW));
                RETURN NEW;
            ELSIF (TG_OP = 'DELETE') THEN
                INSERT INTO change_log (table_name, record_id, change_type, old_data)
                VALUES ('account', OLD.id, 'DELETE', to_jsonb(OLD));
                RETURN OLD;
            END IF;
            RETURN NULL; -- 이 문장은 절대 도달하지 않음
        END;
        $$ LANGUAGE plpgsql;

        -- 트리거 생성
        CREATE TRIGGER account_changes_trigger
        AFTER INSERT OR UPDATE OR DELETE ON account
        FOR EACH ROW EXECUTE FUNCTION log_account_changes();
        ```
3.  **데이터 타입 매핑**: Salesforce와 PostgreSQL 간의 데이터 타입과 필드 이름 매핑을 위한 `sf_field_to_pg_column_name` 및 `pg_column_to_sf_field_name` 함수는 일반적인 규칙을 기반으로 하며, 실제 필드와 데이터 타입에 맞게 커스터마이징이 필요할 수 있습니다. (이전 스키마 매핑 답변 참조!)
4.  **오류 처리 및 재시도**: 예제 코드에는 기본적인 `try-except`만 있지만, 실제 프로덕션에서는 데드 레터 큐(Dead Letter Queue, DLQ), 지수 백오프(Exponential Backoff)를 사용한 재시도 로직, 알림 시스템(Slack, Email 등)이 필수입니다.
5.  **보안**: 데이터베이스 및 Salesforce 자격 증명은 환경 변수나 키 관리 서비스(예: AWS Secrets Manager)를 통해 안전하게 관리해야 합니다. 함수에 부여된 권한도 최소한으로 제한해야 하구요.
6.  **초기 데이터 로드**: 이 이벤트 기반 동기화는 변경 사항만 처리하므로, 최초 시스템 가동 시에는 Airflow DAG을 사용하여 양쪽 시스템의 모든 데이터를 한 번 동기화하는 '초기 로딩' 작업이 필요합니다.

---
제시한 `salesforce_to_postgresql_function.py`와 `postgresql_to_salesforce_function.py`는 **Salesforce와 PostgreSQL 서버가 아닌, 제3의 서버(대부분 클라우드 서비스의 서버리스 플랫폼)**에 배포해서 사용하는 방식이 맞아요! 👏

이렇게 독립적인 서버리스 함수로 운영하면 몇 가지 장점이 있답니다:

*   **분리된 책임 (Decoupling):** Salesforce나 PostgreSQL에 직접적인 부담을 주지 않고, 동기화 로직을 별도로 관리할 수 있어요.
*   **비용 효율성:** 함수가 실행될 때만 비용을 지불하는 서버리스 특성상, 유휴 시간에는 비용이 거의 발생하지 않아요. Heroku Connect의 고정 비용 대비 훨씬 저렴할 수 있죠!
*   **확장성:** 트래픽이 많아지면 자동으로 스케일 아웃되어 대량의 변경 이벤트도 안정적으로 처리할 수 있어요.
*   **유연성:** AWS Lambda, Google Cloud Functions, Azure Functions 등 다양한 클라우드 환경에서 자유롭게 선택하여 배포할 수 있습니다.

---

### 📈 **양방향 동기화 설정/모니터링 웹 서비스 설계**

이제 의 다음 목표인 "동기화 설정 및 시간대별 동기화 건수 추이 표시"를 위한 웹 서비스를 만들어볼게요. 이건 마치 우리만의 미니 대시보드를 만드는 것과 같아요! 완전 멋질 것 같아요! 😉

#### 🚀 **전체 아키텍처 (Frontend + Backend + DB + Monitoring)**

이 웹 서비스는 다음과 같은 구성 요소로 이루어질 거예요.

1.  **Frontend (UI):** React, Vue, Angular 등 JavaScript 프레임워크 (여기서는 개념적으로 React를 가정)
2.  **Backend (API Server):** Node.js (Express), Python (FastAPI/Flask) 등 (여기서는 Python FastAPI를 가정)
    *   동기화 설정 관리 (CRUD: 생성, 읽기, 업데이트, 삭제)
    *   동기화 메트릭 조회 및 집계
3.  **설정 데이터베이스 (Configuration DB):** PostgreSQL, MongoDB 등 (여기서는 PostgreSQL을 가정)
    *   어떤 Salesforce 객체와 PostgreSQL 테이블을 어떤 방향으로 동기화할지 설정 정보 저장.
4.  **메트릭 데이터베이스 (Metrics DB):** PostgreSQL, InfluxDB 등 (여기서는 PostgreSQL을 가정)
    *   동기화가 발생할 때마다 서버리스 함수에서 기록하는 메트릭(동기화 건수 등) 저장.
5.  **모니터링 및 로깅:** 각 서버리스 함수는 동기화 성공/실패 건수를 메트릭 데이터베이스에 기록해야 합니다.

#### 💡 **동작 방식 핵심 아이디어**

*   **동기화 설정**: 웹 서비스에서 사용자가 Salesforce 객체(예: `Account`)와 PostgreSQL 테이블(예: `public.account`)을 맵핑하고, 동기화 방향(Salesforce -> PG, PG -> Salesforce, 또는 양방향)을 설정하면, 이 정보가 설정 데이터베이스에 저장돼요.
*   **서버리스 함수와의 연동**: 동기화 서버리스 함수는 이 설정 데이터베이스를 읽어서 동기화 대상 객체/테이블을 인지하고 해당 로직을 수행합니다.
*   **메트릭 기록**: 서버리스 함수는 동기화 작업이 성공적으로 완료될 때마다 동기화된 레코드 수와 방향 정보를 메트릭 데이터베이스에 기록해요.
*   **추이 시각화**: 웹 서비스의 프론트엔드는 백엔드 API를 통해 메트릭 데이터를 조회하고, 시간대별로 동기화 건수를 차트 형태로 보여주는 거죠!

---

### 💻 **웹 서비스 구성 요소별 예제 코드 스케치**

이건 전체 웹 앱을 만드는 과정이라 모든 코드를 다 보여드리긴 어렵지만, 각 핵심 컴포넌트의 역할을 이해할 수 있는 개념적인 코드 스케치를 제공할게요!

#### 1. **설정/메트릭 데이터베이스 스키마 (PostgreSQL)**

```sql
-- 동기화 설정 테이블
CREATE TABLE sync_configurations (
    id SERIAL PRIMARY KEY,
    sf_object_name VARCHAR(255) NOT NULL, -- Salesforce 객체 이름 (예: 'Account')
    pg_table_name VARCHAR(255) NOT NULL,  -- PostgreSQL 테이블 이름 (예: 'account')
    sync_direction VARCHAR(50) NOT NULL,  -- 'SF_TO_PG', 'PG_TO_SF', 'BIDIRECTIONAL'
    is_active BOOLEAN DEFAULT TRUE,       -- 이 설정이 현재 활성화되어 있는지 여부
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    UNIQUE (sf_object_name, pg_table_name) -- 객체-테이블 쌍은 유니크해야 함
);

-- 동기화 메트릭 테이블
CREATE TABLE sync_metrics (
    id BIGSERIAL PRIMARY KEY,
    sync_config_id INTEGER REFERENCES sync_configurations(id), -- 어떤 설정에 대한 메트릭인지
    direction VARCHAR(50) NOT NULL,       -- 'SF_TO_PG' 또는 'PG_TO_SF'
    synced_count INTEGER NOT NULL,        -- 동기화된 레코드 수
    status VARCHAR(50) NOT NULL,          -- 'SUCCESS', 'FAILURE'
    timestamp TIMESTAMP DEFAULT NOW()
);

-- 변경 이력을 기록하기 위한 테이블 (PostgreSQL -> Salesforce 동기화용)
-- 이건 기존에 PostgreSQL -> Salesforce 함수에서 언급했던 change_log 테이블과 동일
CREATE TABLE change_log (
    id BIGSERIAL PRIMARY KEY,
    table_name TEXT NOT NULL,       
    record_id TEXT NOT NULL,        
    change_type TEXT NOT NULL,      
    old_data JSONB,                 
    new_data JSONB,                 
    processed BOOLEAN DEFAULT FALSE,
    processed_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT NOW()
);
```

#### 2. **Backend API (Python FastAPI 예제)**

FastAPI는 빠르고 쉽게 API 서버를 구축할 수 있어요.

```python
# main.py (FastAPI application)
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import psycopg2
import os
from datetime import datetime, timedelta

app = FastAPI()

# --- 환경 변수 로드 (DB 연결용) ---
PG_HOST = os.environ.get('PG_HOST_WEB') # 웹 서비스용 DB 환경변수 따로 관리 추천
PG_DBNAME = os.environ.get('PG_DBNAME_WEB')
PG_USER = os.environ.get('PG_USER_WEB')
PG_PASSWORD = os.environ.get('PG_PASSWORD_WEB')
PG_PORT = os.environ.get('PG_PORT_WEB', '5432')

# Pydantic 모델: API 요청/응답 데이터 유효성 검사
class SyncConfigBase(BaseModel):
    sf_object_name: str
    pg_table_name: str
    sync_direction: str # 'SF_TO_PG', 'PG_TO_SF', 'BIDIRECTIONAL'
    is_active: bool = True

class SyncConfigCreate(SyncConfigBase):
    pass

class SyncConfig(SyncConfigBase):
    id: int
    created_at: datetime
    updated_at: datetime

    class Config:
        orm_mode = True # SQLAlchemy 같은 ORM과 함께 쓸 때 편리

# DB 연결 헬퍼 함수
def get_db_connection():
    try:
        conn = psycopg2.connect(
            host=PG_HOST, dbname=PG_DBNAME, user=PG_USER, password=PG_PASSWORD, port=PG_PORT
        )
        return conn
    except psycopg2.Error as e:
        print(f"DB Connection Error: {e}")
        raise HTTPException(status_code=500, detail="Database connection error")

# --- Sync Configuration API ---
@app.get("/api/sync-configs", response_model=List[SyncConfig])
async def get_sync_configurations():
    conn = get_db_connection()
    cur = conn.cursor()
    cur.execute("SELECT id, sf_object_name, pg_table_name, sync_direction, is_active, created_at, updated_at FROM sync_configurations ORDER BY sf_object_name;")
    configs = cur.fetchall()
    cur.close()
    conn.close()
    return [SyncConfig(id=c[0], sf_object_name=c[1], pg_table_name=c[2], 
                       sync_direction=c[3], is_active=c[4], created_at=c[5], updated_at=c[6]) for c in configs]

@app.post("/api/sync-configs", response_model=SyncConfig)
async def create_sync_configuration(config: SyncConfigCreate):
    conn = get_db_connection()
    cur = conn.cursor()
    try:
        cur.execute(
            "INSERT INTO sync_configurations (sf_object_name, pg_table_name, sync_direction, is_active) VALUES (%s, %s, %s, %s) RETURNING id, sf_object_name, pg_table_name, sync_direction, is_active, created_at, updated_at;",
            (config.sf_object_name, config.pg_table_name, config.sync_direction, config.is_active)
        )
        new_config = cur.fetchone()
        conn.commit()
        return SyncConfig(id=new_config[0], sf_object_name=new_config[1], pg_table_name=new_config[2],
                          sync_direction=new_config[3], is_active=new_config[4], created_at=new_config[5], updated_at=new_config[6])
    except psycopg2.errors.UniqueViolation:
        raise HTTPException(status_code=400, detail="Configuration for this Salesforce object and PostgreSQL table already exists.")
    except Exception as e:
        conn.rollback()
        raise HTTPException(status_code=500, detail=f"Failed to create configuration: {e}")
    finally:
        cur.close()
        conn.close()

# @app.put("/api/sync-configs/{config_id}") ... (생략 - 업데이트 로직)
# @app.delete("/api/sync-configs/{config_id}") ... (생략 - 삭제 로직)

# --- Sync Metrics API ---
@app.get("/api/sync-metrics/trends")
async def get_sync_trends(
    sf_object_name: Optional[str] = None,
    direction: Optional[str] = None, # 'SF_TO_PG', 'PG_TO_SF'
    interval_hours: int = 24, # 최근 24시간
    group_by_interval: str = '1 hour' # 시간 단위로 그룹화
):
    conn = get_db_connection()
    cur = conn.cursor()

    time_cutoff = datetime.now() - timedelta(hours=interval_hours)
    
    # 쿼리 필터링 조건 동적 생성
    where_clauses = ["sm.timestamp >= %s"]
    params = [time_cutoff]

    if sf_object_name:
        where_clauses.append("sc.sf_object_name = %s")
        params.append(sf_object_name)
    if direction:
        where_clauses.append("sm.direction = %s")
        params.append(direction)
    
    where_str = "WHERE " + " AND ".join(where_clauses) if where_clauses else ""

    # 시간대별 집계 쿼리 (PostgreSQL date_trunc 사용)
    query = f"""
        SELECT 
            date_trunc('{group_by_interval}', sm.timestamp) AS time_bucket,
            sm.direction,
            COALESCE(SUM(sm.synced_count), 0) AS total_synced_count
        FROM sync_metrics sm
        JOIN sync_configurations sc ON sm.sync_config_id = sc.id
        {where_str}
        GROUP BY time_bucket, sm.direction
        ORDER BY time_bucket, sm.direction;
    """
    
    cur.execute(query, tuple(params))
    trends = cur.fetchall()
    cur.close()
    conn.close()

    # 데이터 프론트엔드에서 사용하기 좋게 변환
    result = {}
    for row in trends:
        time_bucket_str = row[0].isoformat() # 시간대를 ISO 포맷 문자열로
        direction_key = row[1]
        count = row[2]
        
        if time_bucket_str not in result:
            result[time_bucket_str] = {}
        result[time_bucket_str][direction_key] = count
    
    # 차트 라이브러리가 요구하는 형태로 변환 (예: [{"time": "...", "SF_TO_PG": 10, "PG_TO_SF": 5}, ...])
    formatted_trends = []
    # 시간 순서대로 데이터를 채우기 위해, 조회 범위의 모든 시간 버킷을 생성하는 로직 추가 필요 (생략)
    for time_bucket, counts in sorted(result.items()):
        formatted_trends.append({
            "time": time_bucket,
            "SF_TO_PG": counts.get("SF_TO_PG", 0),
            "PG_TO_SF": counts.get("PG_TO_SF", 0),
        })

    return formatted_trends
```

#### 3. **서버리스 함수에서의 메트릭 기록 추가 (예: `salesforce_to_postgresql_function.py` 수정)**

이전에 만들었던 서버리스 함수 내에 메트릭 DB에 기록하는 로직을 추가합니다.

```python
# salesforce_to_postgresql_function.py (메트릭 기록 로직 추가 부분)
# ... (기존 코드 생략)

def handler(event, context):
    # ... (기존 초기화 및 파싱 로직 생략)
    pg_conn_metrics = None # 메트릭 DB용 연결
    try:
        # ... (기존 SF -> PG 동기화 로직)
        
        # --- 메트릭 DB에 기록 ---
        if sf_id: # 성공적으로 동기화된 경우
            pg_conn_metrics = psycopg2.connect( # 메트릭 DB 연결 정보
                host=os.environ.get('PG_HOST_METRICS'),
                dbname=os.environ.get('PG_DBNAME_METRICS'),
                user=os.environ.get('PG_USER_METRICS'),
                password=os.environ.get('PG_PASSWORD_METRICS'),
                port=os.environ.get('PG_PORT_METRICS', '5432')
            )
            pg_cursor_metrics = pg_conn_metrics.cursor()

            # 먼저 sync_configurations 테이블에서 해당 설정을 찾아야 합니다.
            # 이 로직은 함수의 시작 부분에서 미리 로드하거나, 매번 쿼리할 수 있습니다.
            # 편의상 여기서 쿼리하는 예시를 보입니다.
            pg_cursor_metrics.execute(
                "SELECT id FROM sync_configurations WHERE sf_object_name = %s AND pg_table_name = %s;",
                (object_name, pg_table_name)
            )
            sync_config_id_row = pg_cursor_metrics.fetchone()
            if sync_config_id_row:
                sync_config_id = sync_config_id_row[0]
                pg_cursor_metrics.execute(
                    "INSERT INTO sync_metrics (sync_config_id, direction, synced_count, status) VALUES (%s, %s, %s, %s);",
                    (sync_config_id, 'SF_TO_PG', 1, 'SUCCESS') # 1건 성공으로 가정. 배치 처리 시에는 실제 건수.
                )
                pg_conn_metrics.commit()
                print("Metrics logged successfully to DB.")
            else:
                print(f"WARN: Could not find sync_configuration for {object_name} -> {pg_table_name}. Metrics not logged.")
            pg_cursor_metrics.close()

        # ... (기존 성공 반환)

    except Exception as e:
        print(f"Error occurred: {e}")
        # 오류 발생 시에도 메트릭 기록 (실패 건수) 로직 추가 가능
        if pg_conn_metrics:
            try:
                pg_cursor_metrics = pg_conn_metrics.cursor()
                # 실패 메트릭 기록 (설정 ID를 찾을 수 없을 수도 있음)
                # 예시: 임시로 -1 을 sync_config_id로 사용하거나 별도의 에러 테이블에 기록
                pg_cursor_metrics.execute(
                    "INSERT INTO sync_metrics (sync_config_id, direction, synced_count, status) VALUES (%s, %s, %s, %s);",
                    (None, 'SF_TO_PG', 1, 'FAILURE') # 어떤 설정에 대한 실패인지 특정할 수 없을 때
                )
                pg_conn_metrics.commit()
                pg_cursor_metrics.close()
            except Exception as metric_e:
                print(f"Failed to log error metric: {metric_e}")
        # ... (오류 반환)
    finally:
        # ... (기존 finally 로직)
        if pg_conn_metrics:
            pg_conn_metrics.close()
```

`postgresql_to_salesforce_function.py`에도 유사하게 메트릭 기록 로직을 추가해주면 됩니다. (방향은 `'PG_TO_SF'`)

#### 4. **Frontend UI (개념적인 React 컴포넌트 스케치)**

```jsx
// components/ObjectConfigurationTable.js (설정 관리)
import React, { useEffect, useState } from 'react';
import axios from 'axios';

function ObjectConfigurationTable() {
    const [configs, setConfigs] = useState([]);
    const [newConfig, setNewConfig] = useState({ sf_object_name: '', pg_table_name: '', sync_direction: 'BIDIRECTIONAL', is_active: true });

    useEffect(() => {
        fetchConfigs();
    }, []);

    const fetchConfigs = async () => {
        const response = await axios.get('/api/sync-configs');
        setConfigs(response.data);
    };

    const handleCreateConfig = async () => {
        try {
            await axios.post('/api/sync-configs', newConfig);
            setNewConfig({ sf_object_name: '', pg_table_name: '', sync_direction: 'BIDIRECTIONAL', is_active: true });
            fetchConfigs(); // 목록 새로고침
        } catch (error) {
            alert('Failed to create config: ' + (error.response?.data?.detail || error.message));
        }
    };

    // ... (업데이트, 삭제 로직 생략)

    return (
        <div>
            <h2>동기화 설정 관리</h2>
            <div>
                {/* 입력 폼 */}
                <input type="text" value={newConfig.sf_object_name} onChange={e => setNewConfig({...newConfig, sf_object_name: e.target.value})} placeholder="Salesforce Object" />
                <input type="text" value={newConfig.pg_table_name} onChange={e => setNewConfig({...newConfig, pg_table_name: e.target.value})} placeholder="PostgreSQL Table" />
                <select value={newConfig.sync_direction} onChange={e => setNewConfig({...newConfig, sync_direction: e.target.value})}>
                    <option value="SF_TO_PG">SF -> PG</option>
                    <option value="PG_TO_SF">PG -> SF</option>
                    <option value="BIDIRECTIONAL">양방향</option>
                </select>
                <button onClick={handleCreateConfig}>추가</button>
            </div>
            <table>
                <thead>
                    <tr>
                        <th>SF 객체</th><th>PG 테이블</th><th>방향</th><th>활성</th><th>Actions</th>
                    </tr>
                </thead>
                <tbody>
                    {configs.map(config => (
                        <tr key={config.id}>
                            <td>{config.sf_object_name}</td>
                            <td>{config.pg_table_name}</td>
                            <td>{config.sync_direction}</td>
                            <td>{config.is_active ? '✅' : '❌'}</td>
                            <td><button>수정</button><button>삭제</button></td>
                        </tr>
                    ))}
                </tbody>
            </table>
        </div>
    );
}
export default ObjectConfigurationTable;

// components/SyncTrendGraph.js (메트릭 시각화)
import React, { useEffect, useState } from 'react';
import axios from 'axios';
import { Line } from 'react-chartjs-2'; // chart.js 라이브러리 사용 가정
import {
    Chart as ChartJS,
    CategoryScale,
    LinearScale,
    PointElement,
    LineElement,
    Title,
    Tooltip,
    Legend,
} from 'chart.js';

ChartJS.register(
    CategoryScale,
    LinearScale,
    PointElement,
    LineElement,
    Title,
    Tooltip,
    Legend
);

function SyncTrendGraph() {
    const [trendData, setTrendData] = useState([]);
    const [selectedObject, setSelectedObject] = useState(''); // 특정 객체 필터링

    useEffect(() => {
        fetchTrends();
    }, [selectedObject]);

    const fetchTrends = async () => {
        const params = selectedObject ? { sf_object_name: selectedObject } : {};
        const response = await axios.get('/api/sync-metrics/trends', { params });
        setTrendData(response.data);
    };

    const chartOptions = {
        responsive: true,
        plugins: {
            legend: { position: 'top' },
            title: { display: true, text: '시간대별 동기화 건수 추이' },
        },
        scales: {
            x: { type: 'category', labels: trendData.map(d => new Date(d.time).toLocaleString()) },
            y: { beginAtZero: true, title: { display: true, text: '건수' } },
        },
    };

    const chartData = {
        labels: trendData.map(d => new Date(d.time).toLocaleString()), // 시간 라벨
        datasets: [
            {
                label: 'SF -> PG 동기화',
                data: trendData.map(d => d.SF_TO_PG),
                borderColor: 'rgb(255, 99, 132)',
                backgroundColor: 'rgba(255, 99, 132, 0.5)',
            },
            {
                label: 'PG -> SF 동기화',
                data: trendData.map(d => d.PG_TO_SF),
                borderColor: 'rgb(53, 162, 235)',
                backgroundColor: 'rgba(53, 162, 235, 0.5)',
            },
        ],
    };

    return (
        <div>
            <h2>동기화 건수 추이</h2>
            <select onChange={e => setSelectedObject(e.target.value)} value={selectedObject}>
                <option value="">모든 객체</option>
                {/* 동기화 설정된 객체 목록을 여기에 동적으로 채워넣을 수 있음 */}
                <option value="Account">Account</option>
                <option value="Contact">Contact</option>
            </select>
            <Line options={chartOptions} data={chartData} />
        </div>
    );
}
export default SyncTrendGraph;
```

---

### 📝 **주의사항 및 추가 고려 사항**

*   **배포 환경:** Backend API는 AWS EC2, GCP Compute Engine, Heroku PaaS, 또는 또 다른 서버리스 API Gateway + Lambda/Cloud Run 조합으로 배포할 수 있습니다. Frontend는 AWS S3 + CloudFront, Netlify, Vercel 등에 배포할 수 있구요.
*   **보안:** 웹 서비스에 대한 인증 및 인가 로직을 추가하여 민감한 설정 정보를 보호해야 합니다.
*   **데이터 정합성:** 분산 시스템에서는 데이터 정합성을 보장하는 것이 중요해요. 실패한 동기화 건수에 대한 재시도, 데드 레터 큐(DLQ) 구축, 주기적인 데이터 스캔 및 불일치 해결 로직 등이 필요합니다.
*   **실시간 vs. 준실시간:** 이 아키텍처는 대부분의 경우 준실시간(near real-time) 동기화를 제공해요. Salesforce에서 이벤트 발생 후 서버리스 함수가 트리거되어 PostgreSQL에 반영되기까지, 또는 PostgreSQL `change_log`에 기록된 후 서버리스 함수가 폴링하여 Salesforce에 반영되기까지 약간의 지연이 발생할 수 있습니다.
*   **트리거 오버헤드:** PostgreSQL의 트리거는 데이터 변경 시 추가적인 오버헤드를 발생시킬 수 있어요. 대량의 트랜잭션이 발생하는 경우 성능 모니터링이 필수입니다. 대안으로는 Debezium 같은 CDC 툴을 사용하는 것을 고려할 수 있습니다.

---
와, ! 아주 날카로운 질문이에요! 💡 네, 제가 방금 말씀드린 `salesforce_to_postgresql_function.py`와 `postgresql_to_salesforce_function.py`는 **Salesforce 서버나 PostgreSQL 서버에 직접 올라가는 코드가 아니에요.** 이 친구들은 **클라우드 서비스(제3의 서버)**에서 **서버리스 함수** 형태로 배포되어 실행될 거예요.

이렇게 하면 훨씬 유연하고 비용 효율적으로 운영할 수 있죠! ✨ 우리 의 목표가 "Heroku Connect를 대체하는 저비용 커넥터"였으니까, 이걸 무료로 테스트할 수 있는 방법을 알려드려야죠!

### 🆓 무료로 테스트할 수 있는 환경!

세상에 감사하게도 개발자를 위한 무료 티어가 많답니다! 이 무료 자원들을 조합해서 우리만의 동기화 시스템을 만들고 테스트해볼 수 있어요.

---

#### 1. ☁️ **Salesforce 환경 (Salesforce Developer Edition)**

*   **무료 사용 방법:** Salesforce에서 제공하는 **Developer Edition**은 개발자가 Salesforce 플랫폼을 무료로 학습하고, 애플리케이션을 개발하고 테스트할 수 있도록 해주는 비영구적이지만 강력한 환경이에요.
*   **장점:** 거의 모든 Salesforce 기능을 사용할 수 있고, 만료 기간 없이 영구적으로 사용할 수 있어요. API 접근도 가능하구요.
*   **신청 방법:** `developer.salesforce.com`에 접속해서 "Sign Up"을 통해 무료 Developer Edition을 신청할 수 있어요.
*   **제한 사항:** 데이터 저장 용량 5MB, 파일 저장 용량 20MB, 최대 2명의 사용자 등 제한이 있지만, 테스트용으로는 충분해요. [6][9]
*   **팁:** 만약 5MB 이상의 데이터가 필요하다면, 스크래치 오르그(Scratch Org)를 사용하면 200MB까지 가능하지만, 이는 일반적으로 DX (Developer Experience) 툴과 연동해서 CLI로 관리해야 해서 초기 세팅이 조금 복잡할 수 있어요. 테스트 초기에는 Developer Edition으로도 충분할 거예요! [8]

---

#### 2. 🐘 **PostgreSQL 환경 (Supabase 또는 Neon.tech)**

*   **Heroku PostgreSQL Free Tier?**: 아쉽게도 Heroku는 2022년 말부터 무료 티어가 대부분 사라졌어요. Heroku Connect를 걷어내는 게 목표인데 Heroku 자체를 쓰지 않는 게 좋겠죠! [5][7]
*   **대안 (초강력 추천):**
    *   **Supabase (수파베이스):** PostgreSQL을 기반으로 하는 오픈소스 Firebase 대안이에요. 엄청나게 강력한 무료 티어를 제공해서 개발자들이 많이 사용해요. 호스팅 PostgreSQL DB는 물론이고, 인증, 스토리지 등 다양한 기능을 지원합니다. PostgreSQL `change_log` 테이블과 트리거도 당연히 만들 수 있어요.
    *   **Neon.tech (네온테크):** 역시 서버리스 PostgreSQL을 표방하며 매우 후한 무료 티어를 제공해요. 사용하지 않을 때는 스케일 투 제로(Scale to Zero)가 되어 비용 효율적이에요.
*   **신청 방법:** 각 서비스 웹사이트에서 회원가입 후 쉽게 PostgreSQL 데이터베이스를 생성할 수 있어요. 연결 정보(Host, DB Name, User, Password, Port)를 얻을 수 있습니다.
*   **팁:** 두 서비스 모두 매우 쉽게 시작할 수 있으며, 개발 친화적이에요.

---

#### 3. 🚀 **서버리스 함수 배포 환경 (AWS Lambda 또는 Google Cloud Functions)**

`salesforce_to_postgresql_function.py`와 `postgresql_to_salesforce_function.py`를 배포할 곳이에요.

*   **AWS Lambda (아마존 웹 서비스 람다):**
    *   **무료 사용 방법:** AWS 프리 티어에 포함되어 연간 백만 건의 요청과 40만 GB-초(GB-seconds)의 컴퓨팅 시간을 무료로 제공해요. 웬만한 테스트는 이 안에서 충분히 가능해요.
    *   **장점:** Salesforce CDC와 연동하기 매우 좋아요 (EventBridge를 통한 통합), 안정적이고 확장성 최고!
    *   **배포 방법:** Python 코드를 `.zip` 파일로 묶어서 필요한 라이브러리(e.g., `simple-salesforce`, `psycopg2-binary`, `pandas`)와 함께 업로드해요. `handler` 함수를 지정하면 됩니다.
    *   **트리거 설정:**
        *   **SF -> PG 함수:**
            *   Salesforce CDC Event -> **AWS EventBridge** (Salesforce 파트너 이벤트 버스 설정) -> AWS Lambda 트리거.
            *   또는, Salesforce Outbound Message -> **AWS API Gateway** (REST API 엔드포인트) -> AWS Lambda 트리거.
        *   **PG -> SF 함수:**
            *   **AWS EventBridge Scheduler** (Cron-like 규칙) -> AWS Lambda 트리거. (이벤트 로그 폴링용)
*   **Google Cloud Functions (구글 클라우드 함수):**
    *   **무료 사용 방법:** GCP 프리 티어에서 매월 2백만 호출, 40만 GB-초의 컴퓨팅 시간을 제공해요.
    *   **장점:** Node.js, Python 등 다양한 언어 지원, 사용하기 쉬운 인터페이스.
    *   **배포 방법:** Python 코드를 클라우드 함수로 배포해요.
    *   **트리거 설정:**
        *   **SF -> PG 함수:** HTTP 트리거를 설정하고 Salesforce Outbound Message의 엔드포인트로 사용.
        *   **PG -> SF 함수:** Cloud Scheduler (Cron 작업) -> Cloud Functions 트리거.

*   **팁:** 클라우드 콘솔에서 환경 변수(DB 정보, Salesforce 로그인 정보 등)를 설정할 수 있어요.

---

#### 4. 🌐 **Backend (FastAPI) 배포 환경 (Google Cloud Run 또는 Render)**

Frontend와 Serverless Functions를 연결하고, 동기화 설정/메트릭을 관리할 Backend API 서버예요.

*   **Google Cloud Run:**
    *   **무료 사용 방법:** 월 2백만 요청, 36만 GB-초의 컴퓨팅 시간 등 매우 관대한 무료 티어를 제공해요.
    *   **장점:** 컨테이너 기반으로 Python FastAPI 같은 웹 애플리케이션을 쉽게 배포하고 관리할 수 있어요. 요청이 없을 때는 인스턴스를 0개로 스케일 아웃해서 비용이 발생하지 않아요.
    *   **배포 방법:** FastAPI 애플리케이션을 Docker 이미지로 만들어서 Cloud Run에 배포해요.
*   **Render.com (렌더):**
    *   **무료 사용 방법:** 웹 서비스 및 백그라운드 워커에 대한 무료 티어(월 750시간)를 제공해요. 작은 API 서버 하나 정도는 무료로 돌릴 수 있습니다.
    *   **장점:** GitHub 저장소와 연동하여 코드 푸시 시 자동으로 배포하는 CI/CD 기능이 편리해요.
*   **팁:** 두 서비스 모두 무료 티어만으로도 충분히 백엔드를 테스트할 수 있을 거예요. 데이터베이스 연결 정보는 환경 변수로 설정해 줍니다.

---

#### 5. 🎨 **Frontend (React) 배포 환경 (Vercel 또는 Netlify)**

사용자 인터페이스를 보여줄 웹 서비스예요.

*   **Vercel (버셀):**
    *   **무료 사용 방법:** 개발자 및 소규모 프로젝트를 위한 매우 넉넉한 무료 티어를 제공해요.
    *   **장점:** GitHub 저장소와 연동하여 코드 푸시 시 자동으로 빌드하고 배포해요. 빠르고 쉽습니다.
*   **Netlify (넷리파이):**
    *   **무료 사용 방법:** Vercel과 유사하게 웹 호스팅, CDN, CI/CD 등 강력한 기능을 무료로 제공해요.
    *   **장점:** 역시 GitHub 연동이 뛰어나고, 정적 웹사이트 배포에 최적화되어 있어요.
*   **팁:** 두 서비스 모두 프론트엔드를 배포할 때, 백엔드 API의 URL(Google Cloud Run/Render에서 얻은 도메인)을 `REACT_APP_API_URL` 같은 환경 변수로 주입해서 빌드하면 돼요.

---

### 🤩 **무료 테스트 환경 전체 그림 요약!**

1.  **Salesforce:** `developer.salesforce.com` (무료)
2.  **PostgreSQL:** `supabase.com` 또는 `neon.tech` (무료 티어)
3.  **Serverless Functions:** `aws.amazon.com/lambda` 또는 `cloud.google.com/functions` (무료 티어)
    *   트리거는 AWS EventBridge/API Gateway 또는 GCP Cloud Scheduler/HTTP Trigger
4.  **Backend API:** `cloud.google.com/run` 또는 `render.com` (무료 티어)
5.  **Frontend Web:** `vercel.com` 또는 `netlify.com` (무료 티어)

이렇게 조합하면 모든 컴포넌트를 무료 또는 아주 저렴한 비용으로 배포하고, 우리 만의 Salesforce-PostgreSQL 양방향 동기화 시스템을 직접 만들고 테스트해볼 수 있을 거예요!

참고 자료 

[1] help.salesforce.com - Salesforce Developer Edition Allocations (https://help.salesforce.com/s/articleView?id=xcloud.overview_limits_developer.htm&language=en_US&type=5)
[2] alternatives - meanands/heroku-free-alternatives (https://github.com/meanands/heroku-free-alternatives)
[3] www.qovery.com - Heroku Alternatives: The 10 Best Competitor Platforms for ... (https://www.qovery.com/blog/best-heroku-alternatives)
[4] www.reddit.com - Salesforce free trial (https://www.reddit.com/r/salesforce/comments/q0syl8/salesforce_free_trial/)
[5] northflank.com - Top Heroku alternatives in 2025 | Blog (https://northflank.com/blog/top-heroku-alternatives)
[6] www.quora.com - What are the limitations of a Salesforce free developer's ... (https://www.quora.com/What-are-the-limitations-of-a-Salesforce-free-developers-account)
[7] Free and Paid Options - 10 Best Heroku Alternatives for 2025 - Free and Paid Options (https://signoz.io/comparisons/heroku-alternatives/)
[8] Trailhead - Account limits | Salesforce Trailblazer Community - Trailhead (https://trailhead.salesforce.com/trailblazer-community/feed/0D54V00007T40CjSAJ)
[9] Salesforce - Limitations of a free Developer account - Trailhead - Salesforce (https://trailhead.salesforce.com/trailblazer-community/feed/0D54V00007T471XSAR)
[10] www.reddit.com - Can anyone recommend a heroku alternative? Or was it ... (https://www.reddit.com/r/node/comments/xa5atb/can_anyone_recommend_a_heroku_alternative_or_was/)

